{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Using LSTM"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import BCEWithLogitsLoss, Embedding, LSTM, Linear, Module\n",
        "from torch.optim import Adam\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "# Download and load the name datasets\n",
        "nltk.download('names')\n",
        "names_corpus = nltk.corpus.names\n",
        "male_names = [name for name in names_corpus.words('male.txt')]\n",
        "female_names = [name for name in names_corpus.words('female.txt')]\n",
        "\n",
        "# Create labels for names\n",
        "male_labels = [0] * len(male_names)  # 0 for male\n",
        "female_labels = [1] * len(female_names)  # 1 for female\n",
        "\n",
        "# Combine male and female names and labels\n",
        "all_names = male_names + female_names\n",
        "all_labels = male_labels + female_labels\n",
        "\n",
        "# Shuffle the combined data\n",
        "combined = list(zip(all_names, all_labels))\n",
        "random.shuffle(combined)\n",
        "all_names, all_labels = zip(*combined)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_names, test_names, train_labels, test_labels = train_test_split(all_names, all_labels, test_size=0.2)\n",
        "\n",
        "# Define tokenizer and preprocess data\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(all_names)\n",
        "name_sequences = tokenizer.texts_to_sequences(all_names)\n",
        "padded_names = pad_sequences(name_sequences, maxlen=10)\n",
        "\n",
        "# Convert labels to numpy arrays and tensors\n",
        "all_labels = np.array(all_labels)\n",
        "data_tensor = torch.tensor(padded_names, dtype=torch.long)\n",
        "labels_tensor = torch.tensor(all_labels, dtype=torch.float)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(data_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# DataLoader setup\n",
        "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(list(zip(val_data, val_labels)), batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the NameClassifier neural network model\n",
        "class NameClassifier(Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(NameClassifier, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Including the padding token\n",
        "model = NameClassifier(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128)\n",
        "optimizer = Adam(model.parameters())\n",
        "\n",
        "# Train and validate the model\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_train_preds = 0\n",
        "    total_train_preds = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = BCEWithLogitsLoss()(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        predictions = torch.round(torch.sigmoid(outputs)).squeeze()\n",
        "        correct_train_preds += (predictions == targets).sum().item()\n",
        "        total_train_preds += targets.size(0)\n",
        "\n",
        "    train_accuracy = correct_train_preds / total_train_preds\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_val_preds = 0\n",
        "    total_val_preds = 0\n",
        "\n",
        "    for inputs, targets in val_loader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = BCEWithLogitsLoss()(outputs.squeeze(), targets)\n",
        "            val_loss += loss.item()\n",
        "            predictions = torch.round(torch.sigmoid(outputs)).squeeze()\n",
        "            correct_val_preds += (predictions == targets).sum().item()\n",
        "            total_val_preds += targets.size(0)\n",
        "\n",
        "    val_accuracy = correct_val_preds / total_val_preds\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}, Training Loss: {total_loss / len(train_loader)}, \"\n",
        "          f\"Training Accuracy: {train_accuracy}, Validation Loss: {val_loss / len(val_loader)}, \"\n",
        "          f\"Validation Accuracy: {val_accuracy}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package names to /home/azureuser/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch: 1, Training Loss: 0.45739048210220723, Training Accuracy: 0.7723052714398112, Validation Loss: 0.4202038687467575, Validation Accuracy: 0.8005034612964128\nEpoch: 2, Training Loss: 0.3938017393176879, Training Accuracy: 0.8124311565696302, Validation Loss: 0.4055830296874046, Validation Accuracy: 0.8030207677784771\nEpoch: 3, Training Loss: 0.3592879942164349, Training Accuracy: 0.83147128245476, Validation Loss: 0.3765432271361351, Validation Accuracy: 0.8244178728760226\nEpoch: 4, Training Loss: 0.33306649110125536, Training Accuracy: 0.844846577498033, Validation Loss: 0.37545796543359755, Validation Accuracy: 0.8250471994965387\nEpoch: 5, Training Loss: 0.30687093087027423, Training Accuracy: 0.8599527930763179, Validation Loss: 0.3751748245954514, Validation Accuracy: 0.8231592196349906\nEpoch: 6, Training Loss: 0.2788508547909895, Training Accuracy: 0.8719118804091267, Validation Loss: 0.3644944906234741, Validation Accuracy: 0.8313404657016992\nEpoch: 7, Training Loss: 0.25157208130437525, Training Accuracy: 0.8885916601101495, Validation Loss: 0.3762946128845215, Validation Accuracy: 0.8275645059786029\nEpoch: 8, Training Loss: 0.22795263456938855, Training Accuracy: 0.8994492525570417, Validation Loss: 0.3802767698466778, Validation Accuracy: 0.8363750786658276\nEpoch: 9, Training Loss: 0.20733098171876005, Training Accuracy: 0.9098347757671125, Validation Loss: 0.3814879423379898, Validation Accuracy: 0.8388923851478918\nEpoch: 10, Training Loss: 0.18913521149649692, Training Accuracy: 0.918174665617624, Validation Loss: 0.410215450823307, Validation Accuracy: 0.8281938325991189\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714113492684
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roberta Transformers"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries and Preparing Dataset\n",
        "In this section, we import necessary libraries and download the `names` dataset from the NLTK library. The function `load_and_shuffle_data` is defined to load, label, and shuffle names into male and female categories. This data is then split into training and testing sets.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Download and prepare the dataset of names\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "def load_and_shuffle_data():\n",
        "    \"\"\" Load male and female names from NLTK, label them, and shuffle the dataset. \"\"\"\n",
        "    male_names = [(name, 0) for name in names.words('male.txt')]  # Label 0 for male names\n",
        "    female_names = [(name, 1) for name in names.words('female.txt')]  # Label 1 for female names\n",
        "    combined_data = male_names + female_names\n",
        "    random.shuffle(combined_data)\n",
        "    names_list, labels_list = zip(*combined_data)\n",
        "    return train_test_split(names_list, labels_list, test_size=0.2)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package names to /home/azureuser/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112142747
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Here, we tokenize the names using the `RobertaTokenizerFast` from the Hugging Face library. This step converts text data into a format suitable for model training by handling tasks such as padding and truncation.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_names(names_train, names_test):\n",
        "    \"\"\" Tokenize the training and testing name data. \"\"\"\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "    train_encodings = tokenizer(list(names_train), truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(list(names_test), truncation=True, padding=True)\n",
        "    return train_encodings, test_encodings\n"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112146079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset Class\n",
        "We define a custom PyTorch dataset class `NamesDataset` to handle the tokenized data. This class will facilitate the loading of data during the training and evaluation phases, making it compatible with PyTorch's data handling and batching methodologies.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NamesDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" Custom dataset for handling the names tokenized data. \"\"\"\n",
        "    def __init__(self, tokenized_data, labels):\n",
        "        self.tokenized_data = tokenized_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112149321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  Model Training and Evaluation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(prediction):\n",
        "    \"\"\" Calculate metrics such as accuracy, precision, recall, and F1 score for evaluation. \"\"\"\n",
        "    true_labels = prediction.label_ids\n",
        "    predicted_labels = prediction.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112182405
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    \"\"\" Load data, prepare datasets, train and evaluate the RoBERTa model for sequence classification. \"\"\"\n",
        "    names_train, names_test, labels_train, labels_test = load_and_shuffle_data()\n",
        "    names_train, _, labels_train, _ = train_test_split(names_train, labels_train, test_size=0.5)\n",
        "    names_test, _, labels_test, _ = train_test_split(names_test, labels_test, test_size=0.5)\n",
        "\n",
        "    train_encodings, test_encodings = encode_names(names_train, names_test)\n",
        "    training_dataset = NamesDataset(train_encodings, labels_train)\n",
        "    testing_dataset = NamesDataset(test_encodings, labels_test)\n",
        "    \n",
        "    roberta_classifier = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=roberta_classifier,\n",
        "        args=training_args,\n",
        "        train_dataset=training_dataset,\n",
        "        eval_dataset=testing_dataset,\n",
        "        compute_metrics=calculate_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "    return trainer\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112187212
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/anaconda/envs/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 00:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.390235</td>\n      <td>0.846348</td>\n      <td>0.878000</td>\n      <td>0.876248</td>\n      <td>0.879760</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.430029</td>\n      <td>0.835013</td>\n      <td>0.860192</td>\n      <td>0.920091</td>\n      <td>0.807615</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.375347</td>\n      <td>0.861461</td>\n      <td>0.890656</td>\n      <td>0.883629</td>\n      <td>0.897796</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric eval_loss:\n0.39023515582084656\nAttempted to log scalar metric eval_accuracy:\n0.8463476070528967\nAttempted to log scalar metric eval_f1:\n0.878\nAttempted to log scalar metric eval_precision:\n0.8762475049900199\nAttempted to log scalar metric eval_recall:\n0.8797595190380761\nAttempted to log scalar metric eval_runtime:\n0.1385\nAttempted to log scalar metric eval_samples_per_second:\n5734.561\nAttempted to log scalar metric eval_steps_per_second:\n93.891\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric eval_loss:\n0.43002888560295105\nAttempted to log scalar metric eval_accuracy:\n0.8350125944584383\nAttempted to log scalar metric eval_f1:\n0.8601921024546425\nAttempted to log scalar metric eval_precision:\n0.9200913242009132\nAttempted to log scalar metric eval_recall:\n0.8076152304609219\nAttempted to log scalar metric eval_runtime:\n0.1376\nAttempted to log scalar metric eval_samples_per_second:\n5770.951\nAttempted to log scalar metric eval_steps_per_second:\n94.487\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric eval_loss:\n0.37534669041633606\nAttempted to log scalar metric eval_accuracy:\n0.8614609571788413\nAttempted to log scalar metric eval_f1:\n0.8906560636182903\nAttempted to log scalar metric eval_precision:\n0.883629191321499\nAttempted to log scalar metric eval_recall:\n0.8977955911823647\nAttempted to log scalar metric eval_runtime:\n0.1312\nAttempted to log scalar metric eval_samples_per_second:\n6051.98\nAttempted to log scalar metric eval_steps_per_second:\n99.088\nAttempted to log scalar metric epoch:\n3.0\nAttempted to log scalar metric train_runtime:\n9.8701\nAttempted to log scalar metric train_samples_per_second:\n965.644\nAttempted to log scalar metric train_steps_per_second:\n30.395\nAttempted to log scalar metric total_flos:\n39182991697440.0\nAttempted to log scalar metric train_loss:\n0.42597635904947917\nAttempted to log scalar metric epoch:\n3.0\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric eval_loss:\n0.37534669041633606\nAttempted to log scalar metric eval_accuracy:\n0.8614609571788413\nAttempted to log scalar metric eval_f1:\n0.8906560636182903\nAttempted to log scalar metric eval_precision:\n0.883629191321499\nAttempted to log scalar metric eval_recall:\n0.8977955911823647\nAttempted to log scalar metric eval_runtime:\n0.1579\nAttempted to log scalar metric eval_samples_per_second:\n5027.001\nAttempted to log scalar metric eval_steps_per_second:\n82.306\nAttempted to log scalar metric epoch:\n3.0\neval_loss: 0.37534669041633606\neval_accuracy: 0.8614609571788413\neval_f1: 0.8906560636182903\neval_precision: 0.883629191321499\neval_recall: 0.8977955911823647\neval_runtime: 0.1579\neval_samples_per_second: 5027.001\neval_steps_per_second: 82.306\nepoch: 3.0\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714112199403
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "myenv",
      "language": "python",
      "display_name": "Python (myenv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "myenv"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}